<html>

<head>
<style type="text/css">
.knitr .inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage .left {
  text-align: left;
}
.rimage .right {
  text-align: right;
}
.rimage .center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<title>Quantified Self Movement Data Analysis</title>
</head>

<body>

<p>In my analysis regarding the quantified self movement data yielded a relatively simple model that provided fantastic results. I will describe the data transformations implemented, packages and parameters used, as well as expected sample error and cross validation.</p>

</p>Data Transformation</p>

The first step I took was to set all blank vlaues to NA, then remove columns of all NA values,and did so using the below code.

<div class="chunk" id="unnamed-chunk-1"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">train2</span> <span class="hl kwb">=</span> <span class="hl std">pml.training</span>
<span class="hl std">train2[pml.training</span><span class="hl opt">==</span><span class="hl str">&quot;&quot;</span><span class="hl std">]</span><span class="hl kwb">=</span><span class="hl num">NA</span>
<span class="hl std">train</span> <span class="hl kwb">=</span> <span class="hl std">train2[,</span><span class="hl kwd">colSums</span><span class="hl std">(</span><span class="hl kwd">is.na</span><span class="hl std">(train2))</span><span class="hl opt">&lt;</span><span class="hl num">.8</span><span class="hl opt">*</span><span class="hl kwd">nrow</span><span class="hl std">(train2)]</span>
</pre></div>
</div></div>

This highlighed any columns that contained 80% NA values and removed them. Due to the large number of observations and predictor variables, I was aggressive in this manner. I also removed the X variable , timestamps (raw, cvtd) as well as new_window and num_window. This left 53 feature variables remaining.</p>

</p>Packages and Parameters</p>
My initial use of the caret package proved frustrating due to very long run times and I looked elsewhere, eventually using randomForest. The number of trees used in the voting was 500. This initial model provided very high accuracy of 0.9972.

<div class="chunk" id="unnamed-chunk-2"><div class="rcode"><div class="message"><pre class="knitr r">## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
</pre></div>
</div></div>

<div class="chunk" id="unnamed-chunk-3"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl kwd">library</span><span class="hl std">(</span><span class="hl str">&quot;randomForest&quot;</span><span class="hl std">)</span>
<span class="hl std">RFfit</span> <span class="hl kwb">=</span> <span class="hl kwd">randomForest</span><span class="hl std">(classe</span><span class="hl opt">~</span><span class="hl std">.,</span> <span class="hl kwc">data</span><span class="hl std">=train,</span> <span class="hl kwc">ntree</span><span class="hl std">=</span><span class="hl num">500</span><span class="hl std">)</span>
<span class="hl std">accuracy</span> <span class="hl kwb">=</span> <span class="hl kwd">sum</span><span class="hl std">(RFfit</span><span class="hl opt">$</span><span class="hl std">predicted</span><span class="hl opt">==</span><span class="hl std">train</span><span class="hl opt">$</span><span class="hl std">classe)</span><span class="hl opt">/</span><span class="hl kwd">nrow</span><span class="hl std">(train)</span>
<span class="hl kwd">print</span><span class="hl std">(accuracy)</span>
</pre></div>
<div class="output"><pre class="knitr r">## [1] 0.999949
</pre></div>
</div></div>

<p>This high accuracy led to concerns of over-fitting. Thus, I looked at the Importance of each predictor as computed by randomForest. I then removed the "less" important variables and refir the model. This made very little difference in accuracy and predicted test results.</p>

<div class="chunk" id="unnamed-chunk-4"><div class="rimage default"><img src="figure/unnamed-chunk-4-1.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" class="plot" /></div></div>

</p>Expected Sample Error and Cross Validation</p>
Since the in-sample error was very low, I had a small lower bound on the out of sample error of 0.28%. 
<div class="chunk" id="unnamed-chunk-5"><div class="rimage default"><img src="figure/unnamed-chunk-5-1.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" class="plot" /></div></div>

The package randomForest lets you look at average cross validation error with different numbers of predictor variables. I chose a 4-fold cross validation (mainly because computing time).
<div class="chunk" id="unnamed-chunk-6"><div class="rcode"><div class="source"><pre class="knitr r"><span class="hl std">rfcv.fit</span> <span class="hl kwb">=</span> <span class="hl kwd">rfcv</span><span class="hl std">(train[,</span><span class="hl num">1</span><span class="hl opt">:</span><span class="hl num">53</span><span class="hl std">],train[,</span><span class="hl num">54</span><span class="hl std">],</span><span class="hl kwc">cv.fold</span> <span class="hl std">=</span> <span class="hl num">4</span><span class="hl std">)</span>
<span class="hl std">rfcv.fit</span><span class="hl opt">$</span><span class="hl std">error.cv</span>
</pre></div>
<div class="output"><pre class="knitr r">##         53         26         13          7          3          1 
##   933.6774   775.3896  1069.8067  3034.7335  9426.3456 16634.6613
</pre></div>
</div></div>

</p>Thus, randomForest provided a great model with minimal tweaking of parameters. (it also correctly predicted all 20 of the test cases!)</p>
</body>
</html>
